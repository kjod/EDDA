---
title: "Assignment 4, EDDA 2017"
author: "Martin de la Riva(11403799) and Kieran O'Driscoll(11426438), Group 23"
date: "08 May 2017"
output:
  pdf_document: default 
  html_document: default
highlight: tango
fontsize: 11pt
---
```{r}
library(multcomp)
library(lme4)
library(Rcmdr) #might remove
```

# Assignment 4

## Exercise 1

### 1.
For this exercise  
```{r}
bread_data=read.table("data\\bread.txt", header=TRUE)
#qqnorm(bread_data$hours[bread_data$humidity == "dry" & bread_data$environment == "cold"])
#hist(bread_data$hours[bread_data$humidity == "dry" & bread_data$environment == "cold"])
#2 way anova
I=nrow(unique(bread_data['environment']))
J=nrow(unique(bread_data['hours']));
N=3 #number of tests per experiment
randomization = rbind(rep(1:I,each=N*J),rep(1:J,N*I),sample(1:(N*I*J))) #randomization code
```

### 2.


```{r}
par(mfrow=c(2,2))
boxplot(hours~environment,data=bread_data, main="Plot of hours and environment", 
  	xlab="Environment", ylab="Hours")
boxplot(hours~humidity,data=bread_data, main="Plot of hours and humidity", 
  	xlab="Humidity", ylab="Hours")
attach(bread_data)

interaction.plot(environment, humidity, hours)
interaction.plot(humidity, environment, hours)
```


### 3

```{r}
#Analysisof variance
#2 way alnova
bread_data$environment=as.factor(bread_data$environment)
bread_data$humidity=as.factor(bread_data$humidity)
pvcaov=lm(hours~environment*humidity,data=bread_data)
print(anova(pvcaov))
```


### 4.

```{r}
contrasts(bread_data$environment)=contr.sum
contrasts(bread_data$humidity)=contr.sum
pvcaov2=lm(hours~environment*humidity, data=bread_data)
print(summary(pvcaov2))
```


```{r}
  print(confint(pvcaov2))
```


###5.

```{r}
  qqnorm(residuals(pvcaov2))
```
Not normal so probably contains outliers
```{r}
print(residuals(pvcaov2)) # look for residuals that are outside std
```

```{r}
  cooks.distance(pvcaov2)
  #plot point on graph with cooks distance and levenes distance
  #library(car)  influencePlot(pvcaov2)
```




## Exercise 2
Firstly, we have different measures in the datasets. We need to transform these values in order to have the 3 datasets with the same measures and criteria.  
We will use the 1879-1882 criteria, which is in km/s, and subustracting 299000 to it. In order to do this we will have to transform light3 into this criteria. We know light3 was calculated measuring how many microseconds it takes to perform 7442 km, and afterwards substracting 24.8 milliseconds.

NOTE: We assumed the assignment means milliseconds when talking about Newcomb measurements (24.8 substracted), as after multiplying by 1000 you do get microseconds, and otherways the measurements don't match by any means the speed of light.
```{r}
light1 = scan("light1879.txt")
light2 = scan("light1882.txt")
light3 = scan("light.txt")

# Transform light3 to 1879-1882 criteria
light3 = (light3/1000)+24.8 # milliseconds
light3 = (light3/1000) # seconds to perform 7442 km
light3 = (7442/light3) # in 1 second it will perform x km (transform to km/s)
light3 = light3 - 299000 # final 1879-1882 criteria
```

### 1.
```{r}
par(mfrow=c(1,3))
hist(light1)
hist(light2)
hist(light3)
```
```{r}
par(mfrow=c(1,1))
boxplot(light1, light2, light3)
```

Michelson second experiment coincides with Newcomb measurements, while Michelson's first experiment (light1) differs from the others.  
Although histograms weren't as helpful as we expected them to be, the Boxplots were really useful to see the similarities and differences between the datasets.

### 2.
```{r}
t.test(light1)$conf.int
```
```{r}
t.test(light2)$conf.int
```
```{r}
t.test(light3)$conf.int
```

NOTE: We maintained 1879-1882 criteria (km/sec - 299000). We also tried with only km/sec, but the confidence intervals where exactly the same although adding 299000 to them.

### 3.
Again, 2 and 3 coincide (light3 interval is inside light2 confidence interval values), while 1 differs, having a confidence interval with higher values, and being completely outside of the two other ones.

### 4.
```{r}
most_precise_speed_of_light = 299792.458 - 299000
most_precise_speed_of_light
```

The most precise actual speed of light shows that Michelson's first experiment measures were clearly off, while his second and Newcomb's were much closer to the current accurate solution.

## Exercise 3

### 1.
```{r}
klm = scan("klm.txt")
# First, we check distribution of the data.
klm <- klm[klm<71]
par(mfrow=c(1,2))
hist(klm)
qqnorm(klm)
```

Assuming the maximum delivery duration of the parts is 70 days, we first ignore outliers that are out of that assumption.  
In order to test the median we can know two methods, a sign test or a Wilcoxon test. Looking at the histogram and QQ-plot, we can assume the data to stem from a symmetric population with a certain median. Therefore, we can use Wilcoxon test, which is preferred as it is based on more information about the dataset.  
In order to test wether the median is 32 or less, we will firstly test if it's equal to 32, and if it is rejected, we will check the lower values to see if it can have a lower median.

```{r}
wilcox.test(klm,mu=32)
```

We get a low p-value, and therefore the hypothesis that the median is equal to 32 is rejected. When testing lower values, it can be seen how the p-value decreases, and therefore we also reject the hypothesis that the median of the population may be lower than 32.

```{r}
wilcox.test(klm,mu=31)[[3]]
```
```{r, message=FALSE}
wilcox.test(klm,mu=30)[[3]]
```

### 2.
```{r}
klm = scan("klm.txt")
par(mfrow=c(1,2))
hist(klm)
qqnorm(klm)
```

As it can be seen in the plots, in this case we cannot make the symmetric assumption, and therefore we have to perform a sign Binomial test. 

```{r}
binom.test(sum(klm>70),sum(!!klm),p=0.1)
```

The test returns a confidence interval between 13.38% and 36.04%, and therefore KLM criterion (at most 10% of parts take more than 70 days) is not met.

## Exercise 4

```{r}
par(mfrow=c(1,2))
clouds=read.table("clouds.txt",header=TRUE)

clouds_diff = clouds[,1]-clouds[,2]
hist(clouds_diff)
qqnorm(clouds_diff)
```

### Two-paired test

Data is clearly paired as it arises from the same individual (cloud) at different points in time.

```{r}
t.test(clouds[,1],clouds[,2],paired=TRUE)
```

According to a paired t-test, silver nitrate does have an effect, giving a confidence interval of difference of (8, 547). As it is a confidence interval with positive values, silver nitrate would result in more rain.  
Although this is the result we expected, a Two-Paired test assumes that the sample comes from a normal distribution, which we cannot as it can be seen in the previous plots.

### Mann-Whitney test

```{r}
wilcox.test(clouds[,1],clouds[,2])
```

In order to perform a Mann-Whitney test, we assume that the first sample stems for population F and the second sample from population G, and we test if F=G. As we can make this assumption, the test is appropiate.  
As the p-value is low, we can reject the hypothesis that seeded and unseeded clouds come from the same population.
This means that there is certainly a difference between clouds with silver nitrate and without it.  
The underlying distribution of seeded clouds is shifted to the right from that of unseeded. (i.e. seeded clouds have bigger values than unseeded)

### Kolmogorov-Smirnov test

```{r}
ks.test(clouds[,1],clouds[,2])
```

Again, the assumption for Kolmogorov is similar to Mann-Whitney's and therefore we can perform the test. The p-value is low again and therefore we can conclude that the samples come from different populations, being the mean of seeded clouds higher than those unseeded.

### 2.

```{r}
sqrt_clouds_1 = sqrt(clouds[,1])
sqrt_clouds_2 = sqrt(clouds[,2])
t.test(sqrt_clouds_1,sqrt_clouds_2,paired=TRUE)
wilcox.test(sqrt_clouds_1,sqrt_clouds_2)
ks.test(sqrt_clouds_1,sqrt_clouds_2)
```
For the Two-Paired test we get an smaller confidence interval, but the hypothesis is still rejected, with an even smaller p-value. For Mann-Whitney and Kolmogorov we get the same exact values, for p-values, W (in Mann-Whitney) and D (in Kolmogorov)

### 3.

```{r}
sqrt_clouds_1 = sqrt(sqrt(clouds[,1]))
sqrt_clouds_2 = sqrt(sqrt(clouds[,2]))
t.test(sqrt_clouds_1,sqrt_clouds_2,paired=TRUE)
wilcox.test(sqrt_clouds_1,sqrt_clouds_2)
ks.test(sqrt_clouds_1,sqrt_clouds_2)
```

Again, we get a smallet confidence interval for Two-Paired test with an even smaller p-value, while Mann-Whitney and Kolmogorov maintain their same exact values.

# Assignment 3

## Exercise 1
## Q1
```{r}
data = read.table("data/peruvians.txt", header=TRUE)
pairs(data[,-c(5,6,7)])
```
Based on the diagram above, age, weight and perhaps a case can be made for diastolic. These were chosen because their scatter plots show a cluster that shows the values are in porportion with migration whereas the othe graphs have a scatter plot that don't show any connection with migration.

##Q2
```{r}
 qqnorm(data[['length']])
attach(data)
```

Peruvians is not drawn from a normal distribution so therefore will use Spearmen correlation test to test for correlation.

```{r}
cor.test(age,migration,method="spearman")
```
The p-value recived is 0.002189 which falls under the 0.05 signicant level. Therefore the value is correlated with migration. Since the value is quite low, it can be said there is a high positive correlation as the correlation increases as values get bigger.

```{r}
cor.test(length,migration,method="spearman")
```
With a P-value of 0.6087 it's clear that lenght and migration do not have a correlation.
```{r}
cor.test(wrist,migration,method="spearman")
```
Wrist gets a p-value of 0.1797 which means there is no correlation.
```{r}
cor.test(systolic,migration,method="spearman")
```
Systolic has no correlation with migration, p-value= 0.3054
```{r}
cor.test(diastolic,migration,method="spearman")
```
Diastolic has no correlation, the p-value was 0.6494
```{r}
cor.test(weight,migration,method="spearman")
```
Lastly, weight is correlated with migration having a p-value of 0.02861.
Two of the tree values predicted in question one were correlated, bith weight and age. 
##Exercise 1
##Q1

```{r, echo=FALSE}
data = read.table("data/run.txt", header=TRUE)
par(mfrow=c(1,2)) 
boxplot(before~drink,data=data, main="Energy Drink", 
  	xlab="Engery Drink", ylab="Perfomance Before")
boxplot(after~drink,data=data, main="Energy Drink", 
  	xlab="Engery Drink", ylab="Perfomance After")
#plot(i, migration, main=paste(labels[counter]," vs migration"), xlab=labels[counter], ylab="Migration", pch=19)
#abline(lm(migration~i), col="red") # regression line (y~x) 
```
From the boxplots it indicates that the engery actaully makes students worse after 30 mins and the lemonade makes students better.

```{r, echo=FALSE}
par(mfrow=c(2,2)) 
qqnorm(data[['before']]) 
qqnorm(data[['after']])
hist(data[['before']]) 
hist(data[['after']])

```
Data looks like it was drawn from a normal distribution.  With the exception of the histogram for "after" but there are only 12 entries per drink type which is a small amount to guage whether this is drawn from a normal distribution

##Q2
Using a two paired sample test
```{r, echo=FALSE}
#use t-test
filter = data['drink'] == "energy"
t.test(data['before'][filter],data['after'][filter],paired=TRUE) 
filter = data['drink'] == "lemo"
t.test(data['before'][filter],data['after'][filter],paired=TRUE)
```
For both drinks the p-values obtained are 0.1264 and 0.4373 for "energy" and "lemo" respectively. These p-values fall above the 0.05 range so therefore we cannot reject the hypothesis nor say that there is increase from the before to the after.   

##Q3

Using permuation test permutation for independent samples.

```{r, echo=FALSE}
diff <- data[,1]-data[,2]
lemon_diff <- diff[1:12]
energy_diff <- diff[13:24]
```

```{r, echo=FALSE}
par(mfrow=c(2,2)) 
qqnorm(lemon_diff)
qqnorm(energy_diff)
hist(lemon_diff)
hist(energy_diff)
```
```{r, echo=FALSE}
t.test(lemon_diff, energy_diff)
```
With a high p-value of 0.1586 the null hypothesis can not be rejected therfore there in no meaningful difference between both energy and lemonade.

##Q4
There is only a small sample size, each drink gets allocated 12 people, it could be the case that one group were faster on average therefore would be faster before and after whereas some people might need more than 30 minutes to recover. Also, perhaps a bigger margin then 30 minutes for the gap between before and after. 

##Q5
It would have affected the time difference if people were still tired after the first run.

##Q6
The conclusion is that both the differences for lemonade and energy are drawn from a normal distrubution. The qq plots can be seen in question 3. 
```{r, echo=FALSE}

```

## Exercise 3
##Q1
```{r}
data = read.table("data/dogs.txt", header=TRUE)
par(mfrow=c(2,2))
boxplot(data,data=data)
qqnorm(data[['isofluorane']])
qqnorm(data[['halothane']])
qqnorm(data[['cyclopropane']])
```
Each drug type has 10 examples. All qqplots are close to a normal distribution with the exception of isofluorane which is displayed in top right. It can be presummed that by adding more data that the qqplots would more closely resembles a qqplot of a normal distubution. In this case it is reasonable to assume sample were taken from normal distribution. 


##Q2
Using anova.
```{r}
treats = data.frame(dog=as.vector(as.matrix(data)),treatment=factor(rep(1:3,each=10)))
attach(treats)
pvcaov=lm(dog~treatment,data=treats)
anova(pvcaov)

```

The hypothesis is rejected as the p-value is 0.011 and therefore there is a difference between the treatments. 

```{r}
summary(pvcaov)
```
The mean estimation for $\mu_isofluorane$ is 0.434 and with a p-value of 0.000189.
The estimation for $\mu_halo - \mu_iso$ is 0.035 and with a p-value of 0.807266.
The estimation for $\mu_cyclo - \mu_iso$ is 0.419 and with a p-value of 0.006504.

```{r}
confint(pvcaov)
```

Those are the 95% confidence intervals for $\mu_isofluorane$, $\mu_halo - \mu_iso$ and $\mu_cyclo - \mu_iso$, respectively.

## Q3

```{r}
kruskal.test(dog,treatment,data=treats)
```
The p-value is really close to 0.05, but it is slightly above it, so we cannot reject the hypothesis and therefore we cannot assume that the samples come from different populations. This contrasts with the findings we found with anova test.  

```{r}
treats = data.frame(dog=as.vector(as.matrix(data)),treatment=factor(rep(1:3,each=10)))
attach(treats)
pvcaov=lm(dog~treatment,data=treats)
anova(pvcaov)
qqnorm(pvcaov$residuals)
```



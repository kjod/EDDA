---
title: "Assignment 4, EDDA 2017"
author: "Martin de la Riva(11403799) and Kieran O'Driscoll(11426438), Group 23"
date: "08 May 2017"
output:
  pdf_document: default 
  html_document: default
highlight: tango
fontsize: 11pt
---
```{r}
library(multcomp)
library(lme4)
```

# Assignment 4

## Exercise 1

### 1.
For this exercise  
```{r}
bread_data=read.table("data\\bread.txt", header=TRUE)
I=nrow(unique(bread_data['environment']))
J=nrow(unique(bread_data['hours']));
N=3 #number of tests per experiment
randomization = rbind(rep(1:I,each=N*J),rep(1:J,N*I),sample(1:(N*I*J))) #randomization code
```

### 2.


```{r}
par(mfrow=c(2,2))
boxplot(hours~environment,data=bread_data, main="Plot of hours and environment", 
  	xlab="Environment", ylab="Hours")
boxplot(hours~humidity,data=bread_data, main="Plot of hours and humidity", 
  	xlab="Humidity", ylab="Hours")
attach(bread_data)

interaction.plot(environment, humidity, hours)
interaction.plot(humidity, environment, hours)
```


### 3

```{r}
#Analysisof variance
#2 way alnova
bread_data$environment=as.factor(bread_data$environment)
bread_data$humidity=as.factor(bread_data$humidity)
pvcaov=lm(hours~environment*humidity,data=bread_data)
print(anova(pvcaov))
```


### 4.

```{r}
contrasts(bread_data$environment)=contr.sum
contrasts(bread_data$humidity)=contr.sum
pvcaov2=lm(hours~environment*humidity, data=bread_data)
print(summary(pvcaov2))
```


```{r}
  print(confint(pvcaov2))
```


###5.

```{r}
  qqnorm(residuals(pvcaov2))
```
Not normal so probably contains outliers
```{r}
print(residuals(pvcaov2)) # look for residuals that are outside std
```

```{r}
  cooks.distance(pvcaov2)
  #plot point on graph with cooks distance and levenes distance
  #library(car)  influencePlot(pvcaov2)
```




## Exercise 2


### 1.

```{r}
I=3; B=5; N=15 # 15 students
for (i in 1:B) print(sample(1:(N*I)))

```

### 2.

```{r}
search_data=read.table("data\\search.txt", header=TRUE)
par(mfrow=c(2,2))
boxplot(time~skill,data=search_data, main="Plot of time and skill", 
  	xlab="Skill", ylab="Time")
boxplot(time~interface,data=search_data, main="Plot of time and interface", 
  	xlab="Interface", ylab="Time")
attach(search_data)
interaction.plot(skill, interface, time)
interaction.plot(interface, skill, time)

```
Big correlations!!

### 3.

```{r}
search_data=read.table("data\\search.txt", header=TRUE)#need for reset 
f = (function (x) gsub(" ", "", paste("skill.", x), fixed = TRUE))
search_data['skill'] = apply(search_data['skill'], 1, f)
f = (function (x) gsub(" ", "", paste("interface.", x), fixed = TRUE))
search_data['interface'] = apply(search_data['interface'], 1, f)

temp_data = search_data
temp_data['interface'] = paste("interface", temp_data['interface']) #change to category
new_data = xtabs(time~interface+skill,data=search_data)

aovpen=lm(time~interface+skill,data=search_data)
print(anova(aovpen))
print(summary(aovpen))

```

### 4.
4. Estimate the time it takes a typical user of skill level 4 to find the product
on the website if the website uses interface 3.

```{r}
par(mfrow=c(2,1))
attach(search_data)
interaction.plot(skill,interface,time)
interaction.plot(interface,skill,time) 

```

###5.

```{r}
par(mfrow=c(1,2))
qqnorm(residuals(aovpen))
plot(fitted(aovpen),residuals(aovpen))

```

###6.

```{r}
friedman.test(time,interface,skill)
```

###7.

```{r}
aovpen=lm(time~interface,data=search_data)
print(anova(aovpen))
print(summary(aovpen))
```

###Excercise 3

###1

```{r}
cream_data=read.table("data\\cream.txt", header=TRUE)
#starter+batch+position
cream_data['position'] = factor(cream_data$position)
cream_data['batch'] = factor(cream_data$batch)
cream_data['starter'] = factor(cream_data$starter)
model = lm(acidity???starter+batch+position, data=cream_data)
print(model)
```


###2.
```{r}
pvcmult=glht(model,linfct=mcp(starter="Tukey"))
summary(pvcmult)
```


###3.

```{r}
summary(model)

```

##4.

```{r}
confint(pvcmult)
```

###Excercise 4

###1.
```{r}
cow_data=read.table("data\\cow.txt", header=TRUE)
cow_data$id=factor(cow_data$id)
cow_data$per=factor(cow_data$per)
model=lm(milk~treatment+per+id,data=cow_data)
summary(model)
```

###2.

###3.

```{r}
mixed_model=lmer(milk~treatment+order+per+(1|id),data=cow_data,REML=FALSE)
print(summary(mixed_model))

```


```{r}
mixed_model2=lmer(milk~order+per+(1|id),data=cow_data,REML=FALSE)
anova(mixed_model2,mixed_model)

```

###4.

```{r}
attach(cow_data)
t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
```



# Assignment 5

## Exercise 1

###1.

```{r}
nauseatable=read.table('nauseatable.txt', header=TRUE)

nausea=c()
medicin=c()
for(i in 1:nrow(nauseatable)){
  for(j in 1:ncol(nauseatable)){
    medicin=append(medicin, rep(row.names(nauseatable)[i], nauseatable[i,j]))
    nausea=append(nausea, rep(j-1, nauseatable[i,j]))
  }
}

nausea.frame=data.frame(nausea,medicin)
```

With this code, an appropiate data frame is created from any table of the same characteristics.

###2.
```{r, collapse=TRUE}
nauseatable
xtabs(~medicin+nausea)
```

We can see that the xtabs code makes a table out of a data frame of 2 vectors. With this outcome we can confirm that the transformation of the data.frame is correct.

###3.
```{r}
attach(nausea.frame)
B=1000
tstar=pstar=numeric(B)
for (i in 1:B){
    nausstar=sample(nausea) ## permuting the labels
    tstar[i]=chisq.test(xtabs(~medicin+nausstar))[[1]]
}
myt=chisq.test(xtabs(~medicin+nausea))[[1]]
hist(tstar)
pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
pr
```

Having as $H_0$=The different medicins work equally well against nausea.

Really low chi-square values are registered (mostly less than 2) when permuting the labels. We get a (bootstrap fashion) p-value of around $0.042$. Therefore, we can reject the hypothesis. In other words, the different medicins work differently against nausea.

###4.
```{r}
chisq.test(xtabs(~medicin+nausea))[[3]]
```

Chi-Square test returns a really similar value as the one in the permutation test.

## Exercise 2

###1.
```{r}
airpollution=read.table('airpollution.txt', header=TRUE)
pairs(airpollution)
```

There seems to be clear relation between oxidant with temperature, insolation and wind. There also seems to be related with humidity and day (although not that clear). A linear model looks useful for this data.

###2.
```{r, collapse=TRUE}
summary(lm(oxidant~insolation, data=airpollution))$r.squared#0.2551683
summary(lm(oxidant~humidity, data=airpollution))$r.squared#0.12402
summary(lm(oxidant~temperature, data=airpollution))$r.squared#0.5760164
summary(lm(oxidant~wind, data=airpollution))$r.squared#0.5863157
summary(lm(oxidant~day, data=airpollution))$r.squared#0.01093407
```

As said before, the most relevant variables for the linear model are (in order of importance): wind, temperature, insolation, humidity and day.

Therefore, we will start with a linear model with wind as it's first explanatory variable, and we will add the variables that increase the determinant coefficient until it does not increase anymore (i.e. a step-up method).

```{r, collapse=TRUE}
summary(lm(oxidant~wind+humidity, data=airpollution))$r.squared
summary(lm(oxidant~wind+insolation, data=airpollution))$r.squared
summary(lm(oxidant~wind+day, data=airpollution))$r.squared
summary(lm(oxidant~wind+temperature, data=airpollution))$r.squared
# therefore we add temperature
summary(lm(oxidant~wind+temperature+humidity, data=airpollution))
summary(lm(oxidant~wind+temperature+insolation, data=airpollution))
summary(lm(oxidant~wind+temperature+day, data=airpollution))
# Adding none of the variables yields significance. Therefore we stop at the previous model.
```

Looking at the $R^2$, it increases when adding more variables, although insignificantly when having added the most relevant ones. Investigating the summary and the p-value (using hypothesis $H_0:\beta_i=0$) using the full linear model, we can see that insolation, humidity and day do not apport much information.

From those 3 variables, humidity seems to be the most relevant one, reaching a p-value of 0.131  when having a wind+temperature+humidity model. Besides this, the p-value is above 0.05 and the increase in $R^2$ is still not significant. Therefore, we do not add any more variables to the model, finishing with a oxidant~wind+temperature model.

###3.
```{r, collapse=TRUE}
summary(lm(oxidant~wind+temperature+insolation+humidity+day, data=airpollution))
summary(lm(oxidant~wind+temperature+insolation+humidity, data=airpollution))
summary(lm(oxidant~wind+temperature+humidity, data=airpollution))
summary(lm(oxidant~wind+temperature, data=airpollution))
```

Using a step-down process using the hypothesis $H_0:\beta_i=0$, we get that day, insolation and humidity get extracted from the model. This is as all of them have a p-value higher than 0.05 on every step, as it can be seen above. This leaves a linear model with wind+temperature.

###4.
Final model:

$-5.20334 - 0.42706*wind + 0.52035*temperature + error$

###5.
```{r}
lm1 = summary(lm(oxidant~wind+temperature, data=airpollution))
qqnorm(residuals(lm1))
shapiro.test(residuals(lm1))
```

The normality assumption of the residuals seems correct, and it therefore seems like a correct model.

##Exercise 3

###Finding a model

```{r, collapse=TRUE}
### step-down model
summary(lm(expend~employ+lawyers+pop+bad+crime, data=expensescrime))
# delete crime
summary(lm(expend~employ+lawyers+pop+bad, data=expensescrime))
# delete pop
summary(lm(expend~employ+lawyers+bad, data=expensescrime))
# delete bad
summary(lm(expend~employ+lawyers, data=expensescrime))
# done
```

Using a step-down approach to choose the variables of the linear model, we end up with expend~employ+lawyers. Step-up approach was also tested, leading to the same result.

###Influence points
```{r}
lm2 = lm(expend~employ+lawyers, data=expensescrime)
round(cooks.distance(lm2),2)
plot(1:51, cooks.distance(lm2))
```

It can be clearly seen that the model has 2 influence points: point 5 and 8, with Cook's distances 5.47 and 6.38 respectively.

###Collinearity
```{r}
pairs(lawyers~employ, data=expensescrime)
```

Graphically, a clear collinearity between the variables lawyers and employ can be seen.

```{r}
round(cor(expensescrime[,5:6]),2)
```

Numerically we confirm their collinearity (0.97). Therefore, we should remove one of the variables. Checking the models with both variables, we can decide to keep employ, as it has a higher determination coefficient. We end with a model of the form expend~employ.

```{r}
summary(lm(expend~lawyers, data=expensescrime))
summary(lm(expend~employ, data=expensescrime))
```